import os
from typing import List, Dict, Any

import httpx
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from pydantic import BaseModel

# ------------------------------------------------------------------------------------
# Config
# ------------------------------------------------------------------------------------

RETRIEVER_URL = os.environ.get("RETRIEVER_URL")
if not RETRIEVER_URL:
    print("WARNING: RETRIEVER_URL not set. Orchestrator will not be able to call retriever.")
RETRIEVER_URL = (RETRIEVER_URL or "").rstrip("/")

app = FastAPI(title="Rate Case Orchestrator (Multi-Agent Demo)")


# ------------------------------------------------------------------------------------
# Pydantic models
# ------------------------------------------------------------------------------------

class AskRequest(BaseModel):
    question: str


class SubQueryResult(BaseModel):
    subquery: str
    retriever_answer: str
    supporting_chunks: List[str]


class AgentReview(BaseModel):
    agent_name: str
    status: str
    comments: List[str]


class OrchestratedResponse(BaseModel):
    original_question: str
    subqueries: List[str]
    subquery_results: List[SubQueryResult]
    final_answer: str
    reviews: List[AgentReview]


# ------------------------------------------------------------------------------------
# "Agents" (implemented as helper functions)
# ------------------------------------------------------------------------------------

def decompose_question(question: str) -> List[str]:
    """
    Decomposition Agent:
    Very simple heuristic decomposition for demo purposes.
    - If ' and ' present, split into 2 subquestions.
    - Otherwise, return the full question as a single subquery.
    """
    q = question.strip()
    if not q:
        return []

    if " and " in q.lower():
        parts = q.split(" and ", 1)
        sub1 = parts[0].strip()
        sub2 = parts[1].strip().rstrip("?")
        subqs: List[str] = []
        if sub1:
            subqs.append(sub1 + "?")
        if sub2:
            subqs.append(sub2 + "?")
        return subqs

    return [q]


async def call_retriever(subquery: str, k: int = 5) -> Dict[str, Any]:
    """
    Retrieval Agent:
    Calls your existing retriever service's /ask endpoint.
    Expected retriever API:
        POST /ask  { "question": str, "k": int }
        -> { "answer": str, "supporting_chunks": [chunk_id, ...] }
    """
    if not RETRIEVER_URL:
        raise RuntimeError("RETRIEVER_URL is not configured.")

    url = f"{RETRIEVER_URL}/ask"
    payload = {"question": subquery, "k": k}

    async with httpx.AsyncClient(timeout=60.0) as client:
        resp = await client.post(url, json=payload)
        if resp.status_code != 200:
            raise RuntimeError(
                f"Retriever returned {resp.status_code}: {resp.text}"
            )
        return resp.json()


def synthesize_answer(question: str, sub_results: List[SubQueryResult]) -> str:
    """
    Synthesis Agent:
    Combines the per-subquery retriever answers into one narrative.
    For demo: simple concatenation with headings.
    """
    lines: List[str] = []
    lines.append("### Final Synthesized Answer")
    lines.append("")
    lines.append(f"**Original question:** {question}")
    lines.append("")

    for idx, r in enumerate(sub_results, start=1):
        lines.append(f"**Sub-question {idx}:** {r.subquery}")
        lines.append(r.retriever_answer.strip() or "(no answer returned)")
        lines.append("")

    lines.append(
        "This answer was synthesized from multiple sub-questions. "
        "In a production setup, this synthesis could be driven by a more advanced LLM-based agent."
    )

    return "\n".join(lines)


def drafting_agent_review(sub_results: List[SubQueryResult], final_answer: str) -> AgentReview:
    """
    Drafting Agent:
    High-level check that the underlying response appears to come from an LLM
    (your retriever now prefixes answers with `[Generated by <model> @ <location>]`).
    """
    comments: List[str] = []

    # Look for our Gemini prefix in either the sub-answers or the final synthesized text
    uses_llm = False
    found_models: List[str] = []

    marker = "[Generated by "
    for r in sub_results:
        txt = r.retriever_answer or ""
        if marker in txt:
            uses_llm = True
            # Try to grab the model name if present
            start = txt.find(marker)
            end = txt.find("]", start)
            if start != -1 and end != -1:
                found_models.append(txt[start + len(marker): end])

    if marker in final_answer:
        uses_llm = True

    if uses_llm:
        status = "pass"
        if found_models:
            comments.append(
                f"Answer narrative drafted by: {', '.join(set(found_models))}."
            )
        else:
            comments.append("Answer narrative appears to be drafted by an LLM (Gemini).")
        comments.append(
            "In production, this agent could compare multiple model drafts and pick the best one."
        )
    else:
        status = "needs_review"
        comments.append(
            "Could not detect an LLM drafting marker. Verify that retriever is calling Gemini as expected."
        )

    return AgentReview(
        agent_name="Drafting Agent (Gemini)",
        status=status,
        comments=comments,
    )


def response_validator_agent(sub_results: List[SubQueryResult], final_answer: str) -> AgentReview:
    """
    Response Validator Agent:
    Very light checks for demo purposes.
    """
    comments: List[str] = []

    if not final_answer.strip():
        comments.append("Final answer is empty.")
        status = "needs_changes"
    else:
        if len(sub_results) == 0:
            comments.append("No subquery results were available.")
            status = "needs_changes"
        else:
            comments.append("Answer contains content for at least one sub-question.")
            status = "pass"

    return AgentReview(
        agent_name="Response Validator Agent",
        status=status,
        comments=comments,
    )


def business_reviewer_agent(final_answer: str) -> AgentReview:
    """
    Business Reviewer Agent:
    Stubbed for demo, but you can enrich prompts later.
    """
    comments: List[str] = [
        "High-level check: answer mentions O&M and test year concepts.",
        "For production, this agent would validate alignment with accounting/financial guidelines."
    ]
    status = "pass" if "O&M" in final_answer or "operating" in final_answer.lower() else "needs_review"

    return AgentReview(
        agent_name="Business Reviewer Agent",
        status=status,
        comments=comments,
    )


def legal_reviewer_agent(final_answer: str) -> AgentReview:
    """
    Legal Reviewer Agent:
    Stubbed for demo. In a full system, this would compare against legal/regulatory narratives.
    """
    comments: List[str] = [
        "Automated legal pass: no explicit contradictory regulatory statements detected in this simple check.",
        "In production, this would cross-check against testimony, orders, and regulatory narratives."
    ]
    # Always "needs_review" in demo to show that human review is recommended
    status = "needs_review"

    return AgentReview(
        agent_name="Legal Reviewer Agent",
        status=status,
        comments=comments,
    )


# ------------------------------------------------------------------------------------
# FastAPI endpoints
# ------------------------------------------------------------------------------------

@app.get("/health")
def health():
    return {
        "status": "ok",
        "retriever_url": RETRIEVER_URL or "(missing)",
    }


@app.post("/answer", response_model=OrchestratedResponse)
async def answer(req: AskRequest):
    if not req.question or not req.question.strip():
        raise HTTPException(status_code=400, detail="question must be non-empty")

    # 1) Decomposition Agent
    subqueries = decompose_question(req.question)
    if not subqueries:
        raise HTTPException(status_code=400, detail="failed to decompose question")

    # 2) Retrieval Agents (one call to retriever per subquery)
    sub_results: List[SubQueryResult] = []
    for sq in subqueries:
        try:
            retriever_resp = await call_retriever(sq, k=5)
            retriever_answer = retriever_resp.get("answer", "")
            supporting_chunks = retriever_resp.get("supporting_chunks", [])
        except Exception as e:
            retriever_answer = f"[Error calling retriever: {e}]"
            supporting_chunks = []

        sub_results.append(
            SubQueryResult(
                subquery=sq,
                retriever_answer=retriever_answer,
                supporting_chunks=supporting_chunks,
            )
        )

    # 3) Synthesis Agent
    final_answer = synthesize_answer(req.question, sub_results)

    # 4) Drafting + Validator + Business + Legal Agents
    reviews: List[AgentReview] = []
    reviews.append(drafting_agent_review(sub_results, final_answer))
    reviews.append(response_validator_agent(sub_results, final_answer))
    reviews.append(business_reviewer_agent(final_answer))
    reviews.append(legal_reviewer_agent(final_answer))

    return OrchestratedResponse(
        original_question=req.question,
        subqueries=subqueries,
        subquery_results=sub_results,
        final_answer=final_answer,
        reviews=reviews,
    )

